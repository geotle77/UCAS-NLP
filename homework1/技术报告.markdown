## 报告摘要
本次报告使用的语言是Python，环境是Anaconda，使用的库包括`math`,`beautifulsoup`,`request`,`jieba`等。


## 预测试
对于这次作业，我预先爬取了通过爬取网站 https://5000yan.com/ （五千言国学网）得到四大名著进行初步的熵计算。

这个过程包括：
- 爬取网页内容
- 清洗网页内容，过滤冗余信息（主要是通过保留特定的段div实现）
- 清洗得到的文本，包括删除标点符号和多余的空白
- 单独计算熵值后，合并后再次计算熵值。

结果如下：
![example1](./test/pics/freq_of4.png)

## 中文网页爬取
我选择爬取的网站说人民日报网，该网站的网址比较规则，很容易从中爬取每日的报道内容，其中记录了2020.1.1-至今的所有新闻，通过爬取这一部分的内容就可以采集到大量的文本。

首先对网址的处理,人民日报网每日的报道规则性较强，都是不同的日期加上相同后缀，所以只需要不断生成这样的网址就可以爬取每日的新闻。人民日报连接：http://paper.people.com.cn/rmrb/html/2022-03/26/nbs.D110000renmrb_01.htm
### 网页链接采集和爬取
总的来说，分为以下几步：
- 将其中涉及日期的部分更改为要爬取的日期。
```Python
url = base_url + parse_day.isoformat()[:-3] + "/" + parse_day.isoformat()[8:10] + "/nbs.D110000renmrb_01.htm"
```
- 在每个页面中使用Beautifulsoup来查找HTML中所有的\<a>标签，这些标签位于类名为swiper-slide的div元素内，而这个div元素又位于类名为swiper-container的div元素内。它将所有这些\<a>标签的href属性（即链接）收集到一个列表next_pages中。
```Python
next_pages = [a['href'] for a in soup.select("div.swiper-container div.swiper-slide a")]
page_links = base_url + parse_day.isoformat()[:-3] + "/" + parse_day.isoformat()[8:10] + "/"
```

- 每一期日报中都有若干文章，所以需要从中提取出每篇文章的链接
```Python
        for next_page in next_pages:
            page_link = page_links + next_page
            html = get_html(page_link)
            soup = BeautifulSoup(html, 'html.parser')
            next_passages = [a['href'] for a in soup.select("ul.news-list li a")]
```
- 最后根据每篇文章的链接爬取对应网页中的内容，并保存在txt文件中
```Python
 for next_passage in next_passages:
                next_passage_url = page_links + next_passage
                html = get_html(next_passage_url)
                soup = BeautifulSoup(html, 'html.parser')

                for article in soup.select('div.article'):
                    title = article.select_one('h1').get_text(strip=True)
                    author = article.select_one('p.sec').get_text(strip=True)
                    contents = [p.get_text(strip=True) for p in article.select('div#ozoom p')]
                    print(f"saving {title}")
                    with open('chinese.txt', 'a', encoding='utf-8') as f:
                        f.write(title + '\n' + author + '\n' + '\n'.join(contents) + '\n\n')
```
### 具体细节实现
#### 爬虫伪装
长时间爬取同一网站，容易被反爬虫程序阻挡，造成断连，因此需要进行伪装，即每次采用不同的headers来获取服务器响应。
```Python

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'referer': 'https://www.google.com/'}

#try to get the content of the page with the different random user agent
user_agent_list = [
    "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
    .
    .
    .
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"   
]

def get_html(url):
    try:
        headers['User-Agent'] = random.choice(user_agent_list)
        r = requests.get(url, headers=headers, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except Exception as e:
        print(e)
        return ""
```
#### 文本量控制
可以通过设置以下代码来实现文本量的采集，爬完全部日期的内容大概会获得约300M的文本量数据。
```Python
start_date = datetime.today().date() - delta
end_date = datetime(2021, 1, 1).date()
```

## 英文网页爬取
英文文本相比中文文本信息量更小，同样的大小下，英文文本的信息量差不多为中文的1/3，所以需要采集更大数据量的文本内容。
