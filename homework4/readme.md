# 文本分类（Text Classification）
## 题目描述
请从新浪或其他门户网上收集一定规模的不同类别文档，进行分类整理，利用不同的特征和不同的分类器实现文本内容分类，并对不 同的方法进行对比实验。
## 背景介绍
文本分类是自然语言处理中的一个重要应用技术，根据文档的内容或主题，自动识别文档所属的预先定义的类别标签。文本分类是很多应用场景的基础，比如垃圾邮件识别，舆情分析，情感识别，新闻自动分类，智能客服机器人的知识库分类等等。本文用标注好的搜狗新闻语料，基于scikit-learn机器学习Python库，将文本分类的完整过程实现一遍。之后再使用深度学习方法来进行文本分类。
### 传统机器学习方法
在进行文本分类时，传统的机器学习方法已经被广泛应用。这些方法通常涉及文本预处理、特征提取和分类器训练。以下是一些常见的传统机器学习方法：
1. **文本预处理**  
在进行文本分类之前，文本预处理是一个重要的步骤，通常包括以下步骤：

- 分词：将文本拆分为单词或子词。
- 去除停用词：移除常见但对分类任务无用的词（如"the", "is", "and"等）。
- 词干提取和词形还原：将单词转换为它们的词干形式（如"running"变为"run"）。
2. **特征提取**  
在文本预处理之后，需要将文本转换为数值特征表示。常见的方法有：

- Bag of Words (BoW)：将每个文档表示为词汇表中每个词的频率。
- TF-IDF (Term Frequency-Inverse Document Frequency)：计算词频和逆文档频率的乘积，减少常见词的权重，增加稀有词的权重。
- 词向量（Word Embeddings）：使用预训练的词向量（如Word2Vec、GloVe）或通过训练数据生成词向量，将单词映射到低维向量空间中。
3. **分类方法**
- 朴素贝叶斯 (Naive Bayes)：一种基于贝叶斯定理的概率分类方法，适用于高维数据的分类。
- 决策树和随机森林 (Decision Trees and Random Forests)：基于树结构的分类方法。
- k近邻算法 (k-Nearest Neighbors, k-NN)：基于距离度量的分类方法。
- 逻辑回归 (Logistic Regression)：一种广义线性模型，适用于二分类任务。

### 深度学习方法
传统机器学习方法将文本分词之后，采用了TF-IDF等特征提取方式对文本进行向量化表示，所产生的文本表示矩阵是一个稀疏矩阵，而深度学习方法采用的词向量一般是一个稠密向量，可以理解为将文本的语义抽象信息嵌入到了一个具体的多维空间中，词之间语义关系可以用向量空间中的范数计算来表示。特别是当数据量较大时，深度学习方法在文本分类任务中表现出色。

1. 词嵌入模型能够将单词映射到低维向量空间中的技术。包括：
- Word2Vec：通过无监督学习，将单词映射到低维向量空间。
- GloVe (Global Vectors for Word Representation)：基于全局词共现矩阵生成词向量。
- FastText：扩展了Word2Vec，能够处理未登录词。
2. 循环神经网络（RNN, Recurrent Neural Networks）
- RNN适用于处理序列数据，能够捕捉序列中的时序信息。

- LSTM (Long Short-Term Memory)：一种特殊的RNN，能够更好地捕捉长距离依赖关系，解决了RNN的梯度消失问题。
- GRU (Gated Recurrent Unit)：一种简化的LSTM，计算效率更高。
3. 卷积神经网络（CNN, Convolutional Neural Networks）
- CNN最初用于图像处理，但也能在文本分类中取得良好效果。通过卷积操作，可以提取文本中的局部特征。

- TextCNN：使用一维卷积核对文本序列进行卷积操作，从而提取特征。
6. 预训练语言模型
预训练语言模型通过在大规模语料上进行预训练，能够学习丰富的语言表示，然后在特定任务上进行微调。 
- BERT：使用掩码语言模型和下一句预测任务进行预训练。

## 数据集准备
使用的是搜狗新闻文本标签，数据集中类似C000008这样的标签是语料的子目录，在网上搜到标签对应的新闻类别，为了便于理解，定义了这个映射词典，并保留原有编号信息。语料并解压至CN_Corpus目录下，解压之后目录结构为：
```
CN_Corpus
└─SogouC.reduced
    └─Reduced
        ├─C000008
        ├─C000010
        ├─C000013
        ├─C000014
        ├─C000016
        ├─C000020
        ├─C000022
        ├─C000023
        └─C000024
```