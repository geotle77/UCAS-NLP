## 报告摘要
本次报告使用的语言是Python，环境是Anaconda，使用的库包括`math`,`beautifulsoup`,`request`,`jieba`等。


## 预测试
对于这次作业，我预先爬取了通过爬取网站 https://5000yan.com/ （五千言国学网）得到四大名著进行初步的熵计算。

这个过程包括：
- 爬取网页内容
- 清洗网页内容，过滤冗余信息（主要是通过保留特定的段div实现）
- 清洗得到的文本，包括删除标点符号和多余的空白
- 单独计算熵值后，合并后再次计算熵值。

结果如下：
![example1](./test/pics/freq_of4.png)

## 中文网页爬取
我选择爬取的网站说人民日报网，该网站的网址比较规则，很容易从中爬取每日的报道内容，其中记录了2020.1.1-至今的所有新闻，通过爬取这一部分的内容就可以采集到大量的文本。

首先对网址的处理,人民日报网每日的报道规则性较强，都是不同的日期加上相同后缀，所以只需要不断生成这样的网址就可以爬取每日的新闻。人民日报连接：http://paper.people.com.cn/rmrb/html/2022-03/26/nbs.D110000renmrb_01.htm
### 网页链接采集和爬取
总的来说，分为以下几步：
- 将其中涉及日期的部分更改为要爬取的日期。
```Python
url = base_url + parse_day.isoformat()[:-3] + "/" + parse_day.isoformat()[8:10] + "/nbs.D110000renmrb_01.htm"
```
- 在每个页面中使用Beautifulsoup来查找HTML中所有的\<a>标签，这些标签位于类名为swiper-slide的div元素内，而这个div元素又位于类名为swiper-container的div元素内。它将所有这些\<a>标签的href属性（即链接）收集到一个列表next_pages中。
```Python
next_pages = [a['href'] for a in soup.select("div.swiper-container div.swiper-slide a")]
page_links = base_url + parse_day.isoformat()[:-3] + "/" + parse_day.isoformat()[8:10] + "/"
```

- 每一期日报中都有若干文章，所以需要从中提取出每篇文章的链接
```Python
        for next_page in next_pages:
            page_link = page_links + next_page
            html = get_html(page_link)
            soup = BeautifulSoup(html, 'html.parser')
            next_passages = [a['href'] for a in soup.select("ul.news-list li a")]
```
- 最后根据每篇文章的链接爬取对应网页中的内容，并保存在txt文件中
```Python
 for next_passage in next_passages:
                next_passage_url = page_links + next_passage
                html = get_html(next_passage_url)
                soup = BeautifulSoup(html, 'html.parser')

                for article in soup.select('div.article'):
                    title = article.select_one('h1').get_text(strip=True)
                    author = article.select_one('p.sec').get_text(strip=True)
                    contents = [p.get_text(strip=True) for p in article.select('div#ozoom p')]
                    print(f"saving {title}")
                    with open('chinese.txt', 'a', encoding='utf-8') as f:
                        f.write(title + '\n' + author + '\n' + '\n'.join(contents) + '\n\n')
```

## 英文网页爬取
英文文本相比中文文本信息量更小，同样的大小下，英文文本的信息量差不多为中文的1/3，所以需要采集更大数据量的文本内容。

我共选择了三个爬取策略，分别是
- [中国日报网](https://www.chinadaily.com.cn/china/59b8d010a3108c54ed7dfc23/)
  
  该网页包含了12年至今所有的报道，共近9000篇报道，只需要不断所引出每篇报道的链接，并爬取其中的内容即可。

- [新华网](https://english.news.cn/)
   
   该网页是新华网英文版，因为没有报道索引页，所以我采取最简单递归索引的策略，找到符合**pattern**格式的网页，并爬取其中的内容即可。因为可能会有重复的链接，所以使用**visited**来保存已爬取过的内容。

- [英文小说网](https://novel.tingroom.com/count.php)

    该网页记录了5356篇英文小说及其内容，其中也包括中文对照部分，因此也可以用来补充中文语料。

分别调用不同的爬虫处理程序，爬到足量的英文数据。

## 具体细节实现
### 爬虫伪装
长时间爬取同一网站，容易被反爬虫程序阻挡，造成断连，因此需要进行伪装，即每次采用不同的headers来获取服务器响应。
```Python

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'referer': 'https://www.google.com/'}

#try to get the content of the page with the different random user agent
user_agent_list = [
    "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
    .
    .
    .
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"   
]

def get_html(url):
    try:
        headers['User-Agent'] = random.choice(user_agent_list)
        r = requests.get(url, headers=headers, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except Exception as e:
        print(e)
        return ""
```
### 文本量控制
- 人民日报网：

    可以通过设置以下代码来实现文本量的采集，爬完全部日期的内容大概会获得约300M的文本量数据。
```Python
start_date = datetime.today().date() - delta
end_date = datetime(2021, 1, 1).date()
```
- 新华网：
  
  通过控制文本量大小和递归深度来控制最终终止条件
```Python
max_size = 1024 * 1024 * 100
```

- 中国日报网：

    通过控制索引页的数量来控制爬取内容的数量
```Python
    start_page = 1
    page_num = start_page
    end_page = 4508
  ```

- 英文小说网：
    
    和中国日报网的控制方法一致



## 文本清洗
 文本清洗的过程非常简单，只需要单独提取出中英文字符对应的编码即可
- 中文使用```[^\u4e00-\u9fa5]```来匹配，英文使用```[^a-zA-Z]```来匹配
  
```Python
if language == 'chinese':
        text = re.sub(r'[^\u4e00-\u9fa5]', '', text)
        return text
    elif language == 'english':
        text = re.sub(r'[^a-zA-Z]', '', text)
        return text
```
- 清除标点符号和多余的空白
```Python
def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)  # 删除所有标点符号
    text = re.sub(r'\d+', '', text)  # 删除所有数字
    text = re.sub(r'\s+', ' ', text)  # 替换多余的空格
    text = re.sub(r'\n+', '\n', text)  # 替换多余的换行符
    text = text.lower()  # 转换为小写
    return text.strip()  # 删除首尾的空格和换行符
```
清洗后得到英文文本和中文文本，我未完全爬取完全部数据，所以中文文本共取得80MB，英文文本取得55MB，我以每次添加2MB求得熵值，并最后绘制柱形图。

## 熵值计算

熵值的计算很简单，只需要调用Count类，然后执行以下代码操作：
```Python
def calculate_entropy(text):
    # Count the frequency of each character
    char_counts = Counter(text)
    # Calculate the frequency of each character
    char_frequencies = {char: count / len(text) for char, count in char_counts.items()}
    # Calculate the entropy of each character and sum them up
    entropy = sum(-p * math.log2(p) for p in char_frequencies.values())
    return entropy
```
注意到以上熵值计算中，我取对数的底为2而非自然对数e，计算得到的结果如下图所示：

### 中文：
熵值变化图如下：

![example2](./pics/chiniese_entropy.png)

字符统计结果如下：

![example3](./pics/chinese_count.png)

### 英文

熵值变化图如下：

![example4](./pics/english_entropy.png)

字符统计结果如下：

![example3](./pics/english_count.png)

可以看到，中英文语料库的频次数据很符合经验与直觉，所得数据结论符合预期，计算得到的中英文熵与 PPT 结论相比差别很小，说明结果具备一定的代表性。
